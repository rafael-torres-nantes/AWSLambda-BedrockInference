# AWS Lambda - Bedrock Inference com Modelos de IA

## ğŸ‘¨â€ğŸ’» Projeto desenvolvido por: 
[Rafael Torres Nantes](https://github.com/rafael-torres-nantes)

## Ãndice

* ğŸ“š ContextualizaÃ§Ã£o do projeto
* ğŸ› ï¸ Tecnologias/Ferramentas utilizadas
* ğŸ–¥ï¸ Funcionamento do sistema
   * ğŸ§© AWS Lambda Handler
   * ğŸ¤– Modelos de IA Suportados
   * ğŸ“Š Gerenciamento de Tokens
* ğŸ”€ Arquitetura da aplicaÃ§Ã£o
* ğŸ“ Estrutura do projeto
* âš™ï¸ ConfiguraÃ§Ã£o e variÃ¡veis de ambiente
* ğŸ“Œ Como executar o projeto
* ğŸ¯ Funcionalidades de inferÃªncia
* ğŸ•µï¸ Dificuldades Encontradas

---

## ğŸ“š ContextualizaÃ§Ã£o do projeto

O projeto **AWSLambda-BedrockInference** Ã© uma soluÃ§Ã£o serverless desenvolvida para **inferÃªncia de modelos de IA generativa** utilizando **AWS Bedrock**. O sistema foi projetado para processar prompts atravÃ©s de mÃºltiplos modelos de IA, incluindo Amazon Nova Pro, Anthropic Claude Haiku e Claude Sonnet, gerando respostas inteligentes e precisas.

A aplicaÃ§Ã£o funciona como uma **AWS Lambda Function** que orquestra chamadas para diferentes modelos de IA, aplicando tÃ©cnicas avanÃ§adas de **prompt engineering** e **gerenciamento de tokens** para otimizar o desempenho e os custos.

## ğŸ› ï¸ Tecnologias/Ferramentas utilizadas

[<img src="https://img.shields.io/badge/Python-3776AB?logo=python&logoColor=white">](https://www.python.org/)
[<img src="https://img.shields.io/badge/AWS_Lambda-FF9900?logo=awslambda&logoColor=white">](https://aws.amazon.com/lambda/)
[<img src="https://img.shields.io/badge/AWS_Bedrock-FF9900?logo=amazonaws&logoColor=white">](https://aws.amazon.com/bedrock/)
[<img src="https://img.shields.io/badge/Amazon_Nova_Pro-FF9900?logo=amazonaws&logoColor=white">](https://aws.amazon.com/bedrock/)
[<img src="https://img.shields.io/badge/Anthropic_Claude-000000?logo=anthropic&logoColor=white">](https://www.anthropic.com/)
[<img src="https://img.shields.io/badge/Boto3-0073BB?logo=amazonaws&logoColor=white">](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html)
[<img src="https://img.shields.io/badge/Visual_Studio_Code-007ACC?logo=visual-studio-code&logoColor=white">](https://code.visualstudio.com/)
[<img src="https://img.shields.io/badge/dotenv-ECD53F?logo=dotenv&logoColor=black">](https://pypi.org/project/python-dotenv/)
[<img src="https://img.shields.io/badge/GitHub-181717?logo=github&logoColor=white">](https://github.com/)

## ğŸ–¥ï¸ Funcionamento do sistema

### ğŸ§© AWS Lambda Handler

O core da aplicaÃ§Ã£o estÃ¡ no lambda_handler.py, que orquestra todo o processo de inferÃªncia:

* **InicializaÃ§Ã£o de modelos**: Instancia diferentes modelos de IA (Nova Pro, Claude Haiku, Claude Sonnet)
* **AplicaÃ§Ã£o de templates**: Utiliza PromptTemplate para estruturar prompts
* **Gerenciamento de tokens**: Implementa controle inteligente via `TokenManager`
* **ExecuÃ§Ã£o de inferÃªncia**: Processa requisiÃ§Ãµes atravÃ©s do `BedrockInferenceService`

### ğŸ¤– Modelos de IA Suportados

* **[Amazon Nova Pro](models/amazon_nova_pro.py)**: Modelo multimodal avanÃ§ado da AWS
* **[Anthropic Claude Haiku](models/anthropic_claude_haiku.py)**: Modelo rÃ¡pido e eficiente para tarefas simples
* **[Anthropic Claude Sonnet](models/anthropic_claude_sonnet.py)**: Modelo balanceado para tarefas complexas

### ğŸ“Š Gerenciamento de Tokens

O `TokenManager` implementa:
- **Contagem precisa** de tokens para otimizaÃ§Ã£o de custos
- **Controle de limites** para diferentes modelos
- **Batching inteligente** para prompts longos

## ğŸ”€ Arquitetura da aplicaÃ§Ã£o

O sistema segue uma arquitetura serverless modular:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Prompt Input  â”‚â”€â”€â”€â–¶â”‚  Lambda Handler  â”‚â”€â”€â”€â–¶â”‚  Model Classes  â”‚
â”‚   (Event Data)  â”‚    â”‚  (Orchestrator)  â”‚    â”‚ (Nova/Claude)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚                        â”‚
                                â–¼                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Response      â”‚â—€â”€â”€â”€â”‚  Bedrock Service â”‚â—€â”€â”€â”€â”‚  Prompt Templateâ”‚
â”‚   (AI Output)   â”‚    â”‚  (AWS Bedrock)   â”‚    â”‚   (Structured)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Estrutura do projeto

A estrutura do projeto Ã© organizada da seguinte maneira:

```
AWSLambda-BedrockInference/
â”œâ”€â”€ lambda_handler.py              # Orquestrador principal da Lambda
â”œâ”€â”€ readme.md                      # DocumentaÃ§Ã£o do projeto
â”œâ”€â”€ .env.example                   # Exemplo de variÃ¡veis de ambiente
â”œâ”€â”€ .gitignore                     # Arquivos ignorados pelo Git
â”œâ”€â”€ controllers/
â”‚   â””â”€â”€ token_manager.py           # Gerenciamento de tokens
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ amazon_nova_pro.py         # Modelo Amazon Nova Pro
â”‚   â”œâ”€â”€ anthropic_claude_haiku.py  # Modelo Claude Haiku
â”‚   â””â”€â”€ anthropic_claude_sonnet.py # Modelo Claude Sonnet
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ bedrock_services.py        # ServiÃ§o principal do Bedrock
â”‚   â””â”€â”€ bedrock_inference.py       # ServiÃ§o de inferÃªncia
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ prompt_template.py         # Templates de prompts
â”œâ”€â”€ tmp/                           # Arquivos temporÃ¡rios
â””â”€â”€ utils/
    â”œâ”€â”€ check_aws.py               # VerificaÃ§Ã£o de credenciais AWS
    â””â”€â”€ import_credentials.py       # ImportaÃ§Ã£o de credenciais
```

## âš™ï¸ ConfiguraÃ§Ã£o e variÃ¡veis de ambiente

O projeto utiliza as seguintes variÃ¡veis de ambiente:

```bash
# Credenciais AWS
AWS_ACCESS_KEY_ID="SUA_CHAVE_DE_ACESSO"
AWS_SECRET_ACCESS_KEY="SUA_CHAVE_SECRETA"
AWS_SESSION_TOKEN="SEU_TOKEN_DE_SESSÃƒO"

# ConfiguraÃ§Ã£o do S3
S3_BUCKET_NAME="NOME_DO_BUCKET"

# IDs dos Modelos Bedrock
ANTHROPIC_CLAUDE_SONNET_MODEL_ID="anthropic.claude-3-sonnet-20240229-v1:0"
ANTHROPIC_CLAUDE_HAIKU_MODEL_ID="anthropic.claude-3-haiku-20240307-v1:0"
AMAZON_NOVA_PRO_MODEL_ID="amazon.nova-pro-v1:0"
```

Copie o arquivo .env.example para .env e configure suas credenciais.

## ğŸ“Œ Como executar o projeto

Para executar o projeto localmente ou fazer deploy, siga as instruÃ§Ãµes abaixo:

1. **Clone o repositÃ³rio:**
   ```bash
   git clone https://github.com/rafael-torres-nantes/AWSLambda-BedrockInference.git
   cd AWSLambda-BedrockInference
   ```

2. **Configure as variÃ¡veis de ambiente:**
   ```bash
   # Copie o arquivo de exemplo
   cp .env.example .env
   
   # Edite o arquivo .env com suas credenciais AWS
   nano .env  # Linux/Mac
   notepad .env  # Windows
   ```

3. **Instale as dependÃªncias:**
   ```bash
   pip install boto3 python-dotenv
   ```

4. **Execute localmente para testes:**
   ```bash
   python lambda_handler.py
   ```

5. **Deploy na AWS Lambda:**
   - Comprima todos os arquivos em um arquivo ZIP
   - FaÃ§a upload na AWS Lambda Console
   - Configure as variÃ¡veis de ambiente no console da AWS
   - Ajuste timeout para pelo menos 5 minutos
   - Configure memÃ³ria para 512MB ou superior

## ğŸ¯ Funcionalidades de inferÃªncia

### Processamento Multi-Modelo
- **MÃºltiplos modelos**: Suporte simultÃ¢neo para Nova Pro, Claude Haiku e Sonnet
- **SeleÃ§Ã£o inteligente**: Escolha automÃ¡tica do modelo baseado no contexto
- **Fallback automÃ¡tico**: Alternativa entre modelos em caso de falha
- **ComparaÃ§Ã£o de resultados**: AnÃ¡lise comparativa entre diferentes modelos

### Templates de Prompt AvanÃ§ados
- **Templates estruturados**: Prompts padronizados para diferentes casos de uso
- **ContextualizaÃ§Ã£o dinÃ¢mica**: AdaptaÃ§Ã£o automÃ¡tica ao domÃ­nio da consulta
- **OtimizaÃ§Ã£o de tokens**: ReduÃ§Ã£o inteligente de tokens desnecessÃ¡rios
- **FormataÃ§Ã£o consistente**: SaÃ­das padronizadas e bem estruturadas

### Monitoramento e Controle
- **Contagem de tokens**: Rastreamento preciso do uso de tokens
- **Logs detalhados**: Monitoramento completo do processo de inferÃªncia
- **MÃ©tricas de performance**: AnÃ¡lise de tempo de resposta e qualidade
- **Controle de custos**: OtimizaÃ§Ã£o automÃ¡tica para reduzir gastos

### IntegraÃ§Ã£o Serverless
- **ExecuÃ§Ã£o sob demanda**: AtivaÃ§Ã£o apenas quando necessÃ¡rio
- **Escalabilidade automÃ¡tica**: Ajuste automÃ¡tico de recursos
- **Baixa latÃªncia**: Resposta rÃ¡pida para consultas simples
- **IntegraÃ§Ã£o nativa**: ComunicaÃ§Ã£o direta com outros serviÃ§os AWS

## ğŸ•µï¸ Dificuldades Encontradas

Durante o desenvolvimento do projeto, algumas dificuldades foram enfrentadas, como:

- **Gerenciamento multi-modelo**: Cada modelo Bedrock possui formatos de requisiÃ§Ã£o especÃ­ficos, exigindo classes especializadas para cada um
- **OtimizaÃ§Ã£o de tokens**: ImplementaÃ§Ã£o de um sistema sofisticado de contagem e otimizaÃ§Ã£o de tokens para diferentes modelos com limites variados
- **IntegraÃ§Ã£o com credenciais**: ConfiguraÃ§Ã£o segura de autenticaÃ§Ã£o AWS em ambiente serverless com diferentes tipos de credenciais
- **Debugging serverless**: ImplementaÃ§Ã£o de logs detalhados para facilitar o debugging em ambiente distribuÃ­do da AWS Lambda
- **Templates de prompt**: Desenvolvimento de templates flexÃ­veis que funcionem efetivamente com diferentes modelos de IA
- **Controle de custos**: Balanceamento entre qualidade das respostas e otimizaÃ§Ã£o de custos de uso dos modelos
- **Timeouts de Lambda**: OtimizaÃ§Ã£o do cÃ³digo para evitar timeouts em inferÃªncias mais complexas